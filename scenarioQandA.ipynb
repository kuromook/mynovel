{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cdc49ec-ceee-4d88-a397-c1c17db52ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# novel_qa.py\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "\n",
    "# ========== モデル設定 ==========\n",
    "LLM_NAME = \"cyberagent/open-calm-small\"\n",
    "EMBED_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc6e45e-77dc-44b6-ab97-c08269ca1b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae702246-2684-4b02-888d-fa90e6bd8538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh1/deepl/mynovel/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/sh1/deepl/mynovel/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ========== モデル設定 ==========\n",
    "LLM_NAME = \"cyberagent/open-calm-small\"\n",
    "EMBED_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ========== 1. LLM & トークナイザー ==========\n",
    "print(\"Loading LLM...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_NAME)\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f90017c-3d1a-4edc-9ba4-a058ed663def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedder...\n",
      "Total chunks: 1171\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0c9ca60a0f4c03a22a718f1c628708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index size: 1171\n"
     ]
    }
   ],
   "source": [
    "# ========== 2. 埋め込みモデル ==========\n",
    "print(\"Loading embedder...\")\n",
    "embedder = SentenceTransformer(EMBED_NAME)\n",
    "\n",
    "# ========== 3. テキスト分割 ==========\n",
    "with open(\"wagahaiwa_nekodearu.txt\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 適度に分割（1チャンク = 約300文字）\n",
    "chunk_size = 300\n",
    "chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "# ========== 4. ベクトル化 & FAISS ==========\n",
    "embeddings = embedder.encode(chunks, show_progress_bar=True)\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "print(f\"FAISS index size: {index.ntotal}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2188721-12fd-4fd8-9aa2-9469a064bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 5. 質問応答関数 ==========\n",
    "def ask(question, top_k=3):\n",
    "    q_emb = embedder.encode([question])\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "    context = \"\\n\".join(chunks[i] for i in I[0])\n",
    "\n",
    "    prompt = f\"\"\"以下の小説を読んで、質問に簡潔に短く日本語で答えてください。\n",
    "\n",
    "{context}\n",
    "\n",
    "質問: {question}\n",
    "回答:\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items() if k != \"token_type_ids\"}\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=80,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        do_sample=True,      # サンプル生成\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.5\n",
    "\n",
    "    )\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"=====================================\")\n",
    "    print(answer)\n",
    "    print(\"=====================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e08bc3-486e-4930-ac25-5d3c4598a7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "以下の小説を読んで、質問に簡潔に短く日本語で答えてください。\n",
      "\n",
      "カード\n",
      "\n",
      "枚折ったそうじゃございませんか」「ええその欠けたところに空也餅くうやもちがくっ付いていましてね」と迷亭はこの質問こそ吾縄張内なわばりうちだと急に浮かれ出す。「色気のない人じゃございませんか、何だって楊子ようじを使わないんでしょう」「今度逢あったら注意しておきましょう」と主人がくすくす笑う。「椎茸で歯がかけるくらいじゃ、よほど歯の性しょうが悪いと思われますが、如何いかがなものでしょう」「善いとは言われますまいな――ねえ迷亭」「善い事はないがちょっと愛嬌あいきょうがあるよ。あれぎり、まだ填つめないところが妙だ。今だに空也餅引掛所ひっかけどころになってるなあ奇観だぜ」「歯を填める小遣こづかいがないの\n",
      "君の悪口を云う事もあるそうだがね」\n",
      "「あの娘がか」\n",
      "「ああ」\n",
      "「怪けしからん奴だ、悪口を云うなんて。第一それじゃ寒月に意いがないんじゃないか」\n",
      "「そこがさ、世の中は妙なもので、自分の好いている人の悪口などは殊更ことさら云って見る事もあるからね」\n",
      "「そんな愚ぐな奴がどこの国にいるものか」と主人は斯様かような人情の機微に立ち入った事を云われても頓とんと感じがない。\n",
      "「その愚な奴が随分世の中にゃあるから仕方がない。現に金田の妻君もそう解釈しているのさ。戸惑とまどいをした糸瓜へちまのようだなんて、時々寒月さんの悪口を云いますから、よっぽど心の中うちでは思ってるに相違ありませんと」\n",
      " 主人はこの不可思議\n",
      "\n",
      "質問: 吾輩とは誰かのことか?\n",
      "回答: 「いやぁ大変だったのよなぁ~!」と答えて下さい。(笑)\n",
      "=====================================\n",
      "=====================================\n",
      "以下の小説を読んで、質問に簡潔に短く日本語で答えてください。\n",
      "\n",
      "カード\n",
      "\n",
      "た分量だけツユの嵩かさが増してくる。ところが茶碗の中には元からツユが八分目這入はいっているから、迷亭の箸にかかった蕎麦の四半分しはんぶんも浸つからない先に茶碗はツユで一杯になってしまった。迷亭の箸は茶碗を去さる五寸の上に至ってぴたりと留まったきりしばらく動かない。動かないのも無理はない。少しでも卸おろせばツユが溢こぼれるばかりである。迷亭もここに至って少し※(「足へん+厨」、第3水準1-92-39)躇ちゅうちょの体ていであったが、たちまち脱兎だっとの勢を以て、口を箸の方へ持って行ったなと思う間まもなく、つるつるちゅうと音がして咽喉笛のどぶえが一二度上下じょうげへ無理に動いたら箸の先の蕎麦は消え\n",
      "恰好かっこうを形かたどって作ったものであろう。それでなくてはあんなに恐しく出来るものではない。\n",
      "「いえ泥棒ではありません。落雲館の生徒です」\n",
      "「うそをつけ。落雲館の生徒が無断で人の庭宅に侵入する奴があるか」\n",
      "「しかしこの通りちゃんと学校の徽章きしょうのついている帽子を被かぶっています」\n",
      "「にせものだろう。落雲館の生徒ならなぜむやみに侵入した」\n",
      "「ボールが飛び込んだものですから」\n",
      "「なぜボールを飛び込ました」\n",
      "「つい飛び込んだんです」\n",
      "「怪けしからん奴だ」\n",
      "「以後注意しますから、今度だけ許して下さい」\n",
      "「どこの何者かわからん奴が垣を越えて邸内に闖入ちんにゅうするのを、そう容易たやすく許されると思\n",
      "\n",
      "質問: 吾輩はどこに向かった?\n",
      "回答: この質問の回答者は、「おふくろさん」「母ちゃん」、「おじいちゃま」。この三人以外の人物はこの質問に答えられない。「親父さんのほうこそ私よりよほど先鋭的だと思うのですがね!」というツッコミが入るのでここで省略するが―――【問】 「おまえら何々をした?」\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "# ========== 6. 実行例 ==========\n",
    "ask(\"吾輩とは誰かのことか？\")\n",
    "ask(\"吾輩はどこに向かった？\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df032ad-1500-4427-960e-144abe4e138e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
